{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import packages to use in the pipeline\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, TimestampType, DateType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get AWS access key from config file: dl.cfg\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# launch spark\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"mapreduce.fileoutputcommitter.algorithm.version\", \"2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Load data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data = 's3a://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json'  #'song_data/*/*/*/*.json'\n",
    "log_data = 's3a://udacity-dend/log_data/2018/11/2018-11-13-events.json'   #'log_data/*/*/*.json'\n",
    "output_data = 's3a://sparkify0823/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "songdf = spark.read.json(song_data)\n",
    "logdf = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Process data into analytics tables using Spark\n",
    "- Fact Table\n",
    "    - songplays - records in log data associated with song plays i.e. records with page NextSong\n",
    "        - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "- Dimension Tables\n",
    "    - users - users in the app\n",
    "        - user_id, first_name, last_name, gender, level\n",
    "    - songs - songs in music database\n",
    "        - song_id, title, artist_id, year, duration\n",
    "    - artists - artists in music database\n",
    "        - artist_id, name, location, latitude, longitude\n",
    "    - time - timestamps of records in songplays broken down into specific units\n",
    "        - start_time, hour, day, week, month, year, weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create songs table\n",
    "songs_table = songdf.select('song_id', 'title', 'artist_id', 'year', 'duration')\\\n",
    "                .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create artists table\n",
    "artists_table = songdf.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude')\\\n",
    "                 .withColumnRenamed('artist_name', 'name')\\\n",
    "                 .withColumnRenamed('artist_location', 'location')\\\n",
    "                 .withColumnRenamed('artist_latitude', 'latitude')\\\n",
    "                 .withColumnRenamed('artist_longitude', 'longitude')\\\n",
    "                 .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter out songplay related log only\n",
    "playlogdf = logdf.filter(logdf.page == 'NextSong')\\\n",
    "       .select('ts', 'userId', 'level', 'song', 'artist', 'sessionId', 'location', 'userAgent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# create users table\n",
    "users_table =logdf.select('userId', 'firstName', 'lastName', 'gender', 'level')\\\n",
    "                  .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+-----+--------------------+------------+---------+--------------------+--------------------+--------------------+----------+\n",
      "|           ts|userId|level|                song|      artist|sessionId|            location|           userAgent|           timestamp|  datetime|\n",
      "+-------------+------+-----+--------------------+------------+---------+--------------------+--------------------+--------------------+----------+\n",
      "|1542069637796|    66| free|             Ja I Ty|          Fu|      514|Harrisburg-Carlis...|\"Mozilla/5.0 (Mac...|2018-11-13 00:40:...|2018-11-13|\n",
      "|1542071549796|    51| free|A Party Song (The...|All Time Low|      510|Houston-The Woodl...|\"Mozilla/5.0 (Win...|2018-11-13 01:12:...|2018-11-13|\n",
      "|1542079142796|     9| free|            Pop-Pop!|   Nik & Jay|      379|Eureka-Arcata-For...|Mozilla/5.0 (Wind...|2018-11-13 03:19:...|2018-11-13|\n",
      "+-------------+------+-----+--------------------+------------+---------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create time table\n",
    "\n",
    "def format_datetime(ts):\n",
    "        return datetime.fromtimestamp(ts/1000.0)\n",
    "# create timestamp column\n",
    "get_timestamp = udf(lambda x: format_datetime(int(x)),TimestampType())\n",
    "# create datetime column\n",
    "get_datetime = udf(lambda x: format_datetime(int(x)), DateType())\n",
    "playlogdf = playlogdf.withColumn('timestamp', get_timestamp(playlogdf.ts))\\\n",
    "                     .withColumn('datetime', get_datetime(playlogdf.ts))\n",
    "playlogdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns to create time table\n",
    "tsdf = playlogdf.select('timestamp').dropDuplicates()\n",
    "time_table = tsdf.withColumn('start_time', tsdf.timestamp)\\\n",
    "                      .withColumn('hour',  hour(tsdf.timestamp))\\\n",
    "                      .withColumn('day',   dayofmonth(tsdf.timestamp))\\\n",
    "                      .withColumn('week',  weekofyear(tsdf.timestamp))\\\n",
    "                      .withColumn('month', month(tsdf.timestamp))\\\n",
    "                      .withColumn('year',  year(tsdf.timestamp))\\\n",
    "                      .withColumn('weekday', dayofweek(tsdf.timestamp))\\\n",
    "                      .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+---+----+-----+----+-------+\n",
      "|           timestamp|          start_time|hour|day|week|month|year|weekday|\n",
      "+--------------------+--------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-13 18:00:...|2018-11-13 18:00:...|  18| 13|  46|   11|2018|      3|\n",
      "|2018-11-13 22:23:...|2018-11-13 22:23:...|  22| 13|  46|   11|2018|      3|\n",
      "|2018-11-13 15:54:...|2018-11-13 15:54:...|  15| 13|  46|   11|2018|      3|\n",
      "+--------------------+--------------------+----+---+----+-----+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songdf.createOrReplaceTempView('songdf_table')\n",
    "playlogdf.createOrReplaceTempView('playlogdf_table')\n",
    "\n",
    "songplays_table = spark.sql('''\n",
    "                               Select row_number() over(order by a.timestamp) as songplay_id,\n",
    "                                      a.timestamp as start_time, \n",
    "                                      a.userID as user_id,\n",
    "                                      a.level,\n",
    "                                      s.song_id,\n",
    "                                      s.artist_id,\n",
    "                                      a.sessionId as session_id,\n",
    "                                      a.location,\n",
    "                                      a.userAgent as user_agent\n",
    "                               from songdf_table s inner join playlogdf_table a\n",
    "                                   on s.title=a.song and a.artist=s.artist_name\n",
    "                            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+-----+-------+---------+----------+--------+----------+\n",
      "|songplay_id|start_time|user_id|level|song_id|artist_id|session_id|location|user_agent|\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------+----------+\n",
      "+-----------+----------+-------+-----+-------+---------+----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## load data back to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table=songs_table.write.partitionBy('year', 'artist_id').parquet(os.path.join(output_data, 'songs'), 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write artists table back to S3\n",
    "artists_table=artists_table.write.parquet(os.path.join(output_data, 'artists'), 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write users table back to S3\n",
    "users_table = users_table.write.parquet(os.path.join(output_data, 'users'), 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write time table to S3 as parquet files\n",
    "time_table = time_table.write.partitionBy('year', 'month').parquet(os.path.join(output_data, 'time'), 'overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table = songplays_table.withColumn(\"year\",year('start_time'))\\\n",
    "                                 .withColumn(\"month\",month('start_time'))\\\n",
    "                                 .write.partitionBy(\"year\",\"month\").parquet(os.path.join(output_data, 'songplays'), 'overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
